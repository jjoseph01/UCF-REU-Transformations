{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjoseph01/UCF-REU-Transformations/blob/main/Real_REU_video_loader_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H15Noz6h5jgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3378c57c-24d5-423a-d34e-53cfa6efb482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jepa' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/jepa.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KYHD2sVF2E-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fff3ce-dada-4d3c-8ddd-93bca9a780ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install decord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NFjZ1WKNKtbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6a7c36-a5f9-4121-f774-b7ae7fb6ceac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jHmYCr40PkOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc09820a-3a69-4d51-954e-45254dc707bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vJYuHHbd9tom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6c5f0b-8cb8-4687-c4aa-6febc13da357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (12.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VXZoe0Rdbxlm"
      },
      "outputs": [],
      "source": [
        "!touch /content/jepa/src/__init__.py\n",
        "!touch /content/jepa/src/datasets/__init__.py\n",
        "!touch /content/jepa/src/datasets/utils/__init__.py\n",
        "!touch /content/jepa/src/datasets/utils/video/__init__.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rfTa3bxB2E57"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/jepa/src')\n",
        "sys.path.append('/content/jepa/evals/video_classification_frozen')\n",
        "sys.path.append('/content/jepa/src/datasets/utils/video')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WboV4Hhs7kIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ccc313-5890-4504-d649-eadc22a3ef2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/jepa/src', '/content/jepa/evals/video_classification_frozen', '/content/jepa/src/datasets/utils/video']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AwwahfIe4Mum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee38c648-2f76-49de-da6d-6b16feee4eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jepa/src\n",
            "datasets  __init__.py  masks  models  __pycache__  utils\n",
            "/content/jepa/src/datasets\n",
            "data_manager.py  image_dataset.py  __init__.py\t__pycache__  utils  video_dataset.py\n",
            "/content/jepa/src/datasets/utils/video\n",
            "functional.py  __pycache__     randerase.py   volume_transforms.py\n",
            "__init__.py    randaugment.py  transforms.py\n"
          ]
        }
      ],
      "source": [
        "%cd /content/jepa/src\n",
        "!ls\n",
        "%cd datasets\n",
        "!ls\n",
        "%cd utils/video\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FxIg_RAecply"
      },
      "outputs": [],
      "source": [
        "!grep -r \"video_transforms\" /content/jepa/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R7PiQpR_bq2l"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/jepa/src')\n",
        "sys.path.append('/content/jepa/src/datasets')\n",
        "sys.path.append('/content/jepa/src/datasets/utils/video')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MJh24ScebY3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48aa100-2954-49a5-cbd8-fdbeab12d50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade torch torchvision timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_orzgqVWKa4x"
      },
      "outputs": [],
      "source": [
        "from timm.data import create_transform as timm_make_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Nlhc4du1JQFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f2103a-8550-4a0c-cdad-03b55b3126cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No module named 'src'\n",
            "Imported DistributedWeightedSampler successfully\n",
            "No module named 'src'\n",
            "Imported timm_make_transforms successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/jepa/src')\n",
        "\n",
        "try:\n",
        "    from datasets.video_dataset import VideoDataset\n",
        "    print(\"Imported VideoDataset successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "try:\n",
        "    from datasets.utils.weighted_sampler import DistributedWeightedSampler\n",
        "    print(\"Imported DistributedWeightedSampler successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "# Import the video transforms from the appropriate path\n",
        "sys.path.append('/content/jepa/src/datasets/utils/video')\n",
        "\n",
        "try:\n",
        "    from datasets.utils.video import transforms as video_transforms #video or vision\n",
        "    from datasets.utils.video import volume_transforms as volume_transforms\n",
        "    print(\"Imported video transforms successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "# Ensure the utils module can be found and import create_transform from timm\n",
        "try:\n",
        "    from timm.data import create_transform as timm_make_transforms\n",
        "    print(\"Imported timm_make_transforms successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fQk6CPnQczIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e29a196-2194-4056-88d2-5ba3e873fb81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported video_transforms successfully\n",
            "Finished importing video_transforms\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import requests\n",
        "\n",
        "# Add the parent directory of 'jepa/src' to the Python path\n",
        "sys.path.append('/content/jepa')\n",
        "\n",
        "# Replace with the actual GitHub Raw URL where volume_transforms is located\n",
        "github_raw_url = \"https://raw.githubusercontent.com/facebookresearch/jepa/main/src/datasets/utils/video/volume_transforms.py\"\n",
        "\n",
        "# Function to download the raw content of a file from GitHub\n",
        "def download_file_from_github_raw(url):\n",
        "    response = requests.get(url)\n",
        "    return response.content.decode('utf-8')\n",
        "\n",
        "# Function to save the content to a file\n",
        "def save_content_to_file(content, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Define the path where you want to save the file\n",
        "file_path = '/content/volume_transforms_custom.py'  # Adjust the path as needed\n",
        "\n",
        "# Download and save the content from GitHub Raw URL to the file\n",
        "content = download_file_from_github_raw(github_raw_url)\n",
        "save_content_to_file(content, file_path)\n",
        "\n",
        "# Now you can import volume_transforms from the downloaded file\n",
        "try:\n",
        "    import video_transforms_custom as video_transforms\n",
        "    print(\"Imported video_transforms successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "# Example usage of video_transforms\n",
        "# Assuming video_transforms has classes/functions like:\n",
        "# transform1 = video_transforms.Transform1()\n",
        "# transform2 = video_transforms.Transform2()\n",
        "# ...\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Finished importing video_transforms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GcsceAKJhLai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb502585-557c-4ba8-8838-ff770d3d9f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/video_transforms_custom.py exists!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if the file exists in the specified path\n",
        "file_path = '/content/video_transforms_custom.py'\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"{file_path} exists!\")\n",
        "else:\n",
        "    print(f\"{file_path} does not exist or is in the wrong directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q0tfN9kfSrQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4674f3a1-e531-4f5b-bcb7-7b55d2de27f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-21 17:14:22--  https://raw.githubusercontent.com/facebookresearch/jepa/main/src/datasets/utils/video/transforms.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40427 (39K) [text/plain]\n",
            "Saving to: ‘/content/video_transforms_custom.py’\n",
            "\n",
            "/content/video_tran 100%[===================>]  39.48K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-06-21 17:14:22 (3.34 MB/s) - ‘/content/video_transforms_custom.py’ saved [40427/40427]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O /content/video_transforms_custom.py https://raw.githubusercontent.com/facebookresearch/jepa/main/src/datasets/utils/video/transforms.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0P_reebTSetp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c8ad5c-4c84-449f-a43d-1ed2b47d9fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported video_transforms successfully\n",
            "Finished importing video_transforms\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import video_transforms_custom as video_transforms\n",
        "    print(\"Imported video_transforms successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Finished importing video_transforms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zr8R7axxg7Ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b8ec0a-24c9-4209-9047-a75b09bd4207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported volume_transforms successfully\n",
            "Finished importing volume_transforms\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import requests\n",
        "\n",
        "# Add the parent directory of 'jepa/src' to the Python path\n",
        "sys.path.append('/content/jepa')\n",
        "\n",
        "# Replace with the actual GitHub Raw URL where volume_transforms is located\n",
        "github_raw_url = \"https://raw.githubusercontent.com/facebookresearch/jepa/main/src/datasets/utils/video/volume_transforms.py\"\n",
        "\n",
        "# Function to download the raw content of a file from GitHub\n",
        "def download_file_from_github_raw(url):\n",
        "    response = requests.get(url)\n",
        "    return response.content.decode('utf-8')\n",
        "\n",
        "# Function to save the content to a file\n",
        "def save_content_to_file(content, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Define the path where you want to save the file\n",
        "file_path = '/content/volume_transforms_custom.py'  # Adjust the path as needed\n",
        "\n",
        "# Download and save the content from GitHub Raw URL to the file\n",
        "content = download_file_from_github_raw(github_raw_url)\n",
        "save_content_to_file(content, file_path)\n",
        "\n",
        "# Now you can import volume_transforms from the downloaded file\n",
        "try:\n",
        "    import volume_transforms_custom as volume_transforms\n",
        "    print(\"Imported volume_transforms successfully\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Finished importing volume_transforms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dAoaTostbC6o"
      },
      "outputs": [],
      "source": [
        "# Create the transform object using timm_make_transforms\n",
        "input_size = 224  # Example input size\n",
        "transform = timm_make_transforms(input_size=input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "r1s0OPYaIQFi"
      },
      "outputs": [],
      "source": [
        "# Update Python path to include the datasets.utils.video directory\n",
        "sys.path.append('/content/jepa/src/datasets/utils/video')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "girgSGB22E3h"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_videodataset(\n",
        "    data_paths,\n",
        "    batch_size,\n",
        "    frames_per_clip=8,\n",
        "    frame_step=4,\n",
        "    num_clips=1,\n",
        "    random_clip_sampling=True,\n",
        "    allow_clip_overlap=False,\n",
        "    filter_short_videos=False,\n",
        "    filter_long_videos=int(10**9),\n",
        "    transform=None,\n",
        "    shared_transform=None,\n",
        "    rank=0,\n",
        "    world_size=1,\n",
        "    datasets_weights=None,\n",
        "    collator=None,\n",
        "    drop_last=True,\n",
        "    num_workers=10,\n",
        "    pin_mem=True,\n",
        "    duration=None,\n",
        "    log_dir=None,\n",
        "):\n",
        "    dataset = VideoDataset(\n",
        "        data_paths=data_paths,\n",
        "        datasets_weights=datasets_weights,\n",
        "        frames_per_clip=frames_per_clip,\n",
        "        frame_step=frame_step,\n",
        "        num_clips=num_clips,\n",
        "        random_clip_sampling=random_clip_sampling,\n",
        "        allow_clip_overlap=allow_clip_overlap,\n",
        "        filter_short_videos=filter_short_videos,\n",
        "        filter_long_videos=filter_long_videos,\n",
        "        duration=duration,\n",
        "        shared_transform=shared_transform,\n",
        "        transform=transform)\n",
        "\n",
        "    logger.info('VideoDataset dataset created')\n",
        "    if datasets_weights is not None:\n",
        "        dist_sampler = DistributedWeightedSampler(\n",
        "            dataset.sample_weights,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "    else:\n",
        "        dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        collate_fn=collator,\n",
        "        sampler=dist_sampler,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=drop_last,\n",
        "        pin_memory=pin_mem,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=num_workers > 0)\n",
        "    logger.info('VideoDataset unsupervised data loader created')\n",
        "\n",
        "    return dataset, data_loader, dist_sampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "IDeQ_YKII7rL",
        "outputId": "e0c70f04-e6d0-491e-b39f-beed28a46d7f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c14f33b-f7ff-4f1c-9c92-b6a115863f3c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c14f33b-f7ff-4f1c-9c92-b6a115863f3c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB_yDqqwjfzX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure the target directory exists\n",
        "target_dir = '/content/data/'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Move the uploaded file to the target directory\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPAKr72vjj0y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('/content/data'))\n",
        "print(uploaded.keys())\n",
        "print(\"Uploaded file keys:\")\n",
        "print(uploaded.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMzKS6iDJBUY"
      },
      "outputs": [],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxsvkyPeoHtx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "import torch\n",
        "import numpy as np\n",
        "from IPython.display import display, clear_output\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dERHErdb2E1S"
      },
      "outputs": [],
      "source": [
        "def load_video_decord(sample, filter_long_videos=int(10**9)):\n",
        "    \"\"\"Load video content using Decord\"\"\"\n",
        "    fname = sample\n",
        "    if not os.path.exists(fname):\n",
        "        warnings.warn(f'video path not found {fname=}')\n",
        "        return [], None\n",
        "\n",
        "    _fsize = os.path.getsize(fname)\n",
        "    if _fsize < 1 * 1024:  # avoid hanging issue\n",
        "        warnings.warn(f'video too short {fname=}')\n",
        "        return [], None\n",
        "    if _fsize > filter_long_videos:\n",
        "        warnings.warn(f'skipping long video of size {_fsize=} (bytes)')\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        vr = VideoReader(fname, num_threads=-1, ctx=cpu(0))\n",
        "        print(f\"Video loaded successfully: {fname}\")\n",
        "        return vr, vr.get_avg_fps()\n",
        "    except Exception as e:\n",
        "        warnings.warn(f'error loading video {fname=}: {e}')\n",
        "        return [], None\n",
        "\n",
        "# def display_video_frames(video_reader, num_frames=10):\n",
        "#     \"\"\"Display video frames using Matplotlib\"\"\"\n",
        "#     if not video_reader:\n",
        "#         print(\"No video data to display\")\n",
        "#         return\n",
        "\n",
        "#     fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "#     for i in range(min(num_frames, len(video_reader))):\n",
        "#         try:\n",
        "#             frame = video_reader[i].asnumpy()\n",
        "#             ax = axes[i] if num_frames > 1 else axes\n",
        "#             ax.imshow(frame)\n",
        "#             ax.set_title(f'Frame {i}')\n",
        "#             ax.axis('off')\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error displaying frame {i}: {e}\")\n",
        "\n",
        "#     plt.show()\n",
        "            # # Clear the previous output before displaying the new frame\n",
        "            # clear_output(wait=True)\n",
        "            # display(plt.gcf())\n",
        "            # plt.show()\n",
        "\n",
        "            # Pause to allow the frame to be displayed properly\n",
        "        #     time.sleep(0.1)\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error displaying frame {i}: {e}\")\n",
        "\n",
        "def display_evenly_spaced_frames(video_reader, num_frames=8):\n",
        "    \"\"\"Display N evenly spaced-out frames using Matplotlib\"\"\"\n",
        "    if not video_reader:\n",
        "        print(\"No video data to display\")\n",
        "        return\n",
        "\n",
        "    total_frames = len(video_reader)\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    print(f\"Total frames in video: {total_frames}\")\n",
        "    print(f\"Selected frame indices: {indices}\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        try:\n",
        "            frame = video_reader[idx].asnumpy()\n",
        "            ax = axes[i] if num_frames > 1 else axes\n",
        "            ax.imshow(frame)\n",
        "            ax.set_title(f'Frame {idx}')\n",
        "            ax.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying frame {idx}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "os.listdir()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file path: {file_path}\")\n",
        "import warnings\n",
        "\n",
        "# Assuming you need the file path here\n",
        "fname = f'/content/data/{file_path}'\n",
        "if not os.path.exists(fname):\n",
        "    warnings.warn(f'video path not found {fname=}')\n",
        "else:\n",
        "    print(f\"File found at: {fname}\")\n",
        "    # Proceed with your video processing code\n",
        "\n",
        "# Load the video\n",
        "video_reader, fps = load_video_decord(fname)\n",
        "\n",
        "# Display the first 10 frames\n",
        "display_evenly_spaced_frames(video_reader, num_frames=8) #or change to 4/8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3XIDu8XUY37"
      },
      "outputs": [],
      "source": [
        "def display_video_frames(video_reader):\n",
        "    \"\"\"Display the first 10 frames using Matplotlib.\"\"\"\n",
        "    if not video_reader:\n",
        "        print(\"No video data to display\")\n",
        "        return\n",
        "\n",
        "    num_frames = 10\n",
        "    total_frames = len(video_reader)\n",
        "\n",
        "    print(f\"Total frames in video: {total_frames}\")\n",
        "\n",
        "    if total_frames < num_frames:\n",
        "        num_frames = total_frames\n",
        "        print(f\"Video has less than 10 frames. Displaying all {num_frames} frames.\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        try:\n",
        "            frame = video_reader[i].asnumpy()\n",
        "            ax = axes[i] if num_frames > 1 else axes\n",
        "            ax.imshow(frame)\n",
        "            ax.set_title(f'Frame {i}')\n",
        "            ax.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying frame {i}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'uploaded' is a dictionary of uploaded files\n",
        "# file_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file path: {file_path}\")\n",
        "\n",
        "# Construct the file path\n",
        "# file_path = os.path.join('/content', uploaded_file_name)\n",
        "print(f\"Constructed file path: {fname}\")\n",
        "\n",
        "# Check if the file exists at the constructed path\n",
        "if not os.path.exists(fname):\n",
        "    warnings.warn(f'Video path not found: {fname}')\n",
        "else:\n",
        "    print(f\"File found at: {fname}\")\n",
        "    # Proceed with your video processing code\n",
        "\n",
        "    # Load the video\n",
        "    video_reader, fps = load_video_decord(fname)\n",
        "\n",
        "    # Display the first 10 frames\n",
        "    display_video_frames(video_reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZR17jPBgRa6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import requests\n",
        "\n",
        "# Add the parent directory of 'jepa/src' to the Python path\n",
        "sys.path.append('/content/jepa')  # Adjust this path based on your directory structure\n",
        "\n",
        "# Replace with the actual GitHub Raw URL where VideoDataset is located\n",
        "github_raw_url = \"https://raw.githubusercontent.com/facebookresearch/jepa/main/src/datasets/video_dataset.py\"\n",
        "\n",
        "# Function to download the raw content of a file from GitHub\n",
        "def download_file_from_github_raw(url):\n",
        "    response = requests.get(url)\n",
        "    return response.content.decode('utf-8')\n",
        "\n",
        "# Function to save the content to a file\n",
        "def save_content_to_file(content, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Define the path where you want to save the file\n",
        "file_path = '/content/video_dataset_custom.py'  # Adjust the path as needed\n",
        "\n",
        "# Download and save the content from GitHub Raw URL to the file\n",
        "content = download_file_from_github_raw(github_raw_url)\n",
        "save_content_to_file(content, file_path)\n",
        "\n",
        "# Now you can import VideoDataset from the downloaded file\n",
        "from video_dataset_custom import VideoDataset\n",
        "\n",
        "# Example usage of VideoDataset\n",
        "data_paths = ['/path/to/videos']\n",
        "dataset = VideoDataset(data_paths=data_paths, frames_per_clip=8)\n",
        "\n",
        "# Continue using dataset as needed in your project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJDLGCZxWfzf"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9kBDtl3Vv0g"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the file name\n",
        "video_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Print the uploaded file name\n",
        "print(f'Uploaded file: {video_filename}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybGwaotDVcU9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the video path in Colab environment\n",
        "video_path = f'/content/{video_filename}'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(video_path):\n",
        "    print(f\"{video_path} exists!\")\n",
        "else:\n",
        "    print(f\"{video_path} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxK4WsNAWNo-"
      },
      "outputs": [],
      "source": [
        "# List all files in the /content directory\n",
        "print(os.listdir('/content'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sicqSEpXRc8"
      },
      "outputs": [],
      "source": [
        "#stop here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anfVp2wC-XSe"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade torchvision\n",
        "\n",
        "video_path = f'/content/{video_filename}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkgXBNv9Gk-2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "import cv2\n",
        "import av\n",
        "from video_dataset_custom import VideoDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "import warnings\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Constants\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "batch_size = 10  # Reduced batch size for memory efficiency\n",
        "video_path = '/content/video1.mp4'\n",
        "\n",
        "# Define the transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Load video using OpenCV\n",
        "def load_video(video_path, max_frames=None):\n",
        "    if not os.path.exists(video_path):\n",
        "        raise RuntimeError(f\"File not found: {video_path}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or (max_frames and frame_count >= max_frames):\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Load and process video frames in smaller batches\n",
        "def process_video_in_batches(video_path, batch_size=10, max_frames=None):\n",
        "    frames = load_video(video_path, max_frames=max_frames)\n",
        "    print(f'Loaded {len(frames)} frames from the video.')\n",
        "\n",
        "    for start_idx in range(0, len(frames), batch_size):\n",
        "        batch_frames = frames[start_idx:start_idx + batch_size]\n",
        "        transformed_video = torch.stack([transform(frame) for frame in batch_frames])\n",
        "\n",
        "        # Visualize some frames\n",
        "        num_frames = min(5, len(batch_frames))\n",
        "        fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "        if num_frames == 1:\n",
        "            axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
        "\n",
        "        for i in range(num_frames):\n",
        "            ax = axes[i]\n",
        "            transformed_frame = transformed_video[i].permute(1, 2, 0).numpy()  # Convert CHW to HWC\n",
        "            transformed_frame = transformed_frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "            transformed_frame = np.clip(transformed_frame, 0, 1)\n",
        "            ax.imshow(transformed_frame)\n",
        "            ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "            ax.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Clear memory after processing each batch\n",
        "        del batch_frames, transformed_video\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Load video using torchvision\n",
        "def load_and_transform_video_torchvision(video_path, batch_size=10):\n",
        "    try:\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
        "        print(\"Video loaded using torchvision.\")\n",
        "\n",
        "        for start_idx in range(0, video.shape[0], batch_size):\n",
        "            batch_frames = video[start_idx:start_idx + batch_size].float() / 255.0  # Scale frames\n",
        "\n",
        "            # Permute dimensions and apply normalization in batches\n",
        "            batch_frames = batch_frames.permute(0, 3, 1, 2)\n",
        "            transformed_batch = torch.stack([T.Normalize(mean=mean, std=std)(frame) for frame in batch_frames])\n",
        "\n",
        "            # Visualize transformed frames\n",
        "            num_frames = min(5, batch_frames.size(0))\n",
        "            fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "            if num_frames == 1:\n",
        "                axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
        "\n",
        "            for i in range(num_frames):\n",
        "                ax = axes[i]\n",
        "                frame = transformed_batch[i].permute(1, 2, 0).cpu().numpy()\n",
        "                frame = frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "                frame = np.clip(frame, 0, 1)\n",
        "                ax.imshow(frame)\n",
        "                ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "                ax.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # Clear memory after processing each batch\n",
        "            del batch_frames, transformed_batch\n",
        "            torch.cuda.empty_cache()\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Load video using Decord\n",
        "def load_video_decord(filepath, max_frames=None):\n",
        "    video_reader = VideoReader(filepath, ctx=cpu(0))\n",
        "    frames = []\n",
        "    for i in range(len(video_reader)):\n",
        "        if max_frames and i >= max_frames:\n",
        "            break\n",
        "        frame = video_reader[i].asnumpy()\n",
        "        frames.append(frame)\n",
        "    return frames\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Process video with OpenCV\n",
        "    process_video_in_batches(video_path, batch_size=batch_size, max_frames=100)\n",
        "\n",
        "    # Process video with torchvision\n",
        "    load_and_transform_video_torchvision(video_path, batch_size=batch_size)\n",
        "\n",
        "    # Process video with Decord\n",
        "    decord_frames = load_video_decord(video_path, max_frames=100)\n",
        "    if decord_frames:\n",
        "        for start_idx in range(0, len(decord_frames), batch_size):\n",
        "            batch_frames = decord_frames[start_idx:start_idx + batch_size]\n",
        "            transformed_video = torch.stack([transform(frame) for frame in batch_frames])\n",
        "\n",
        "            # Visualize some frames\n",
        "            num_frames = min(5, len(batch_frames))\n",
        "            fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "            if num_frames == 1:\n",
        "                axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
        "\n",
        "            for i in range(num_frames):\n",
        "                ax = axes[i]\n",
        "                transformed_frame = transformed_video[i].permute(1, 2, 0).numpy()  # Convert CHW to HWC\n",
        "                transformed_frame = transformed_frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "                transformed_frame = np.clip(transformed_frame, 0, 1)\n",
        "                ax.imshow(transformed_frame)\n",
        "                ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "                ax.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # Clear memory after processing each batch\n",
        "            del batch_frames, transformed_video\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# # Main execution flow\n",
        "# os.listdir()\n",
        "# uploaded_file_path = list(uploaded.keys())[0]\n",
        "# print(f\"Uploaded file path: {uploaded_file_path}\")\n",
        "\n",
        "# # Construct the full file path\n",
        "# fname = f'/content/{uploaded_file_path}'\n",
        "# if not os.path.exists(fname):\n",
        "#     warnings.warn(f'Video path not found: {fname}')\n",
        "# else:\n",
        "#     print(f\"File found at: {fname}\")\n",
        "\n",
        "#     # Load the video\n",
        "#     video_reader, fps = load_video_decord(fname)\n",
        "\n",
        "#     # Display the first 10 frames\n",
        "#     display_video_frames(video_reader, num_frames=1) #originally 10\n",
        "\n",
        "#     # Apply transformations\n",
        "#     transform = get_video_transforms()\n",
        "\n",
        "#     # Step 6: Convert video frames to numpy arrays and apply transformations\n",
        "#     transformed_frames = []\n",
        "#     for frame in video_reader:\n",
        "#         # Convert the frame to torch tensor and permute dimensions\n",
        "#         frame_tensor = torch.tensor(frame.asnumpy()).permute(2, 0, 1).float() / 255.\n",
        "#         # Apply the transformation\n",
        "#         transformed_frame = transform(frame_tensor)\n",
        "#         # Append to transformed_frames\n",
        "#         transformed_frames.append(transformed_frame.numpy())  # No need for .numpy() here\n",
        "\n",
        "#     # Step 7: Stack transformed frames into a single numpy array\n",
        "#     transformed_video = np.stack(transformed_frames)\n",
        "\n",
        "#     # Step 8: Print the transformed video shape\n",
        "#     print(\"Transformed video shape:\", transformed_video.shape)\n",
        "\n",
        "#     # Step 9: Print the transformed video\n",
        "#     print(\"Transformed video:\")\n",
        "#     print(transformed_video)\n",
        "\n",
        "#     fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
        "#     for i in range(10):\n",
        "    #     ax = axes[i]\n",
        "    #     transformed_frame = transformed_video[i].transpose(1, 2, 0)  # Convert CHW to HWC\n",
        "    #     transformed_frame = (transformed_frame * std + mean).clip(0, 1)  # De-normalize for visualization\n",
        "    #     ax.imshow(transformed_frame)\n",
        "    #     ax.set_title(f'Transformed Frame {i}')\n",
        "    #     ax.axis('off')\n",
        "    # plt.show()\n",
        "\n",
        "# Efficient Data Loading and Processing\n",
        "def make_videodataset(\n",
        "    data_paths,\n",
        "    batch_size,\n",
        "    frames_per_clip=8,\n",
        "    frame_step=4,\n",
        "    num_clips=1,\n",
        "    random_clip_sampling=True,\n",
        "    allow_clip_overlap=False,\n",
        "    filter_short_videos=False,\n",
        "    filter_long_videos=int(10**9),\n",
        "    transform=None,\n",
        "    shared_transform=None,\n",
        "    rank=0,\n",
        "    world_size=1,\n",
        "    datasets_weights=None,\n",
        "    collator=None,\n",
        "    drop_last=True,\n",
        "    num_workers=1,  # Set to 2 or 1 to reduce memory usage\n",
        "    pin_mem=True,\n",
        "    duration=None,\n",
        "    log_dir=None,\n",
        "):\n",
        "    dataset = VideoDataset(\n",
        "        data_paths=data_paths,\n",
        "        datasets_weights=datasets_weights,\n",
        "        frames_per_clip=frames_per_clip,\n",
        "        frame_step=frame_step,\n",
        "        num_clips=num_clips,\n",
        "        random_clip_sampling=random_clip_sampling,\n",
        "        allow_clip_overlap=allow_clip_overlap,\n",
        "        filter_short_videos=filter_short_videos,\n",
        "        filter_long_videos=filter_long_videos,\n",
        "        duration=duration,\n",
        "        shared_transform=shared_transform,\n",
        "        transform=transform)\n",
        "\n",
        "    print('VideoDataset dataset created')\n",
        "    if datasets_weights is not None:\n",
        "        dist_sampler = DistributedWeightedSampler(\n",
        "            dataset.sample_weights,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "    else:\n",
        "        dist_sampler = DistributedSampler(\n",
        "            dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        collate_fn=collator,\n",
        "        sampler=dist_sampler,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=drop_last,\n",
        "        pin_memory=pin_mem,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=num_workers > 0)\n",
        "    print('VideoDataset unsupervised data loader created')\n",
        "\n",
        "    return dataset, data_loader, dist_sampler\n",
        "\n",
        "# Use the correct video path\n",
        "dataset, data_loader, dist_sampler = make_videodataset(\n",
        "    data_paths=[fname],\n",
        "    batch_size=2,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Efficiently Process and Transform Video Frames\n",
        "for i, batch in enumerate(data_loader):\n",
        "    print(f\"Processing batch {i}\")\n",
        "    transformed_frames = batch.permute(0, 2, 3, 1).numpy()  # Convert from tensor to numpy for display\n",
        "    for frame in transformed_frames:\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    if i >= 2:  # Limit to processing first 2 batches for demonstration\n",
        "        break\n",
        "\n",
        "\n",
        "# Path to the video file\n",
        "video_path = f'/content/{video_filename}'\n",
        "\n",
        "video, audio, info = read_video(video_path, pts_unit='sec')\n",
        "\n",
        "# Define the transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Process and visualize frames in smaller batches\n",
        "batch_size = 5\n",
        "max_frames = 100  # Limit frames for memory management\n",
        "\n",
        "for start_idx in range(0, min(max_frames, video.shape[0]), batch_size):\n",
        "    batch_video = video[start_idx:start_idx + batch_size]\n",
        "    batch_frames = []\n",
        "\n",
        "    for idx, frame in enumerate(batch_video):\n",
        "        try:\n",
        "            # Ensure frame has the correct shape and type for PIL conversion\n",
        "            frame_np = frame.permute(1, 2, 0).numpy().astype('uint8')  # Convert to numpy array\n",
        "            frame_pil = Image.fromarray(frame_np)\n",
        "\n",
        "            # Optionally, resize and apply transformations if needed\n",
        "            frame_pil = transform(frame_pil)\n",
        "\n",
        "            batch_frames.append(frame_pil)\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting frame {start_idx + idx} to PIL Image: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not batch_frames:\n",
        "        print(f\"No valid frames found in batch {start_idx // batch_size}. Skipping batch.\")\n",
        "        continue\n",
        "\n",
        "    # Stack transformed frames into a tensor\n",
        "    normalized_video = torch.stack(batch_frames)\n",
        "\n",
        "    # Display transformed video shape and a few frames\n",
        "    print(f\"Transformed video shape for batch {start_idx // batch_size}:\", normalized_video.shape)\n",
        "    print(f\"Transformed video for batch {start_idx // batch_size}:\", normalized_video[:3])\n",
        "\n",
        "    # Optionally, visualize the frames using matplotlib or display them in some other way\n",
        "    # Ensure to handle visualization or further processing as per your application's needs\n",
        "\n",
        "    # Clear memory after processing each batch\n",
        "    del batch_video, batch_frames, normalized_video\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YaMUzLYqQ31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from decord import VideoReader, cpu\n",
        "from video_dataset_custom import VideoDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "# Constants\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "batch_size = 10\n",
        "video_path = '/content/video1.mp4'\n",
        "\n",
        "# Define the transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Load video using OpenCV\n",
        "def load_video(video_path, max_frames=None):\n",
        "    if not os.path.exists(video_path):\n",
        "        raise FileNotFoundError(f\"File not found: {video_path}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or (max_frames and frame_count >= max_frames):\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Process video frames in smaller batches\n",
        "def process_video_in_batches(video_path, batch_size=10, max_frames=None):\n",
        "    frames = load_video(video_path, max_frames=max_frames)\n",
        "    print(f'Loaded {len(frames)} frames from the video.')\n",
        "\n",
        "    for start_idx in range(0, len(frames), batch_size):\n",
        "        batch_frames = frames[start_idx:start_idx + batch_size]\n",
        "        transformed_video = torch.stack([transform(Image.fromarray(frame)) for frame in batch_frames])\n",
        "\n",
        "        # Visualize some frames\n",
        "        num_frames = min(5, len(batch_frames))\n",
        "        fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "        if num_frames == 1:\n",
        "            axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
        "\n",
        "        for i in range(num_frames):\n",
        "            ax = axes[i]\n",
        "            transformed_frame = transformed_video[i].permute(1, 2, 0).numpy()  # Convert CHW to HWC\n",
        "            transformed_frame = transformed_frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "            transformed_frame = np.clip(transformed_frame, 0, 1)\n",
        "            ax.imshow(transformed_frame)\n",
        "            ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Clear memory after processing each batch\n",
        "        del batch_frames, transformed_video\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Load and transform video using torchvision\n",
        "def load_and_transform_video_torchvision(video_path, batch_size=10):\n",
        "    try:\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
        "        print(\"Video loaded using torchvision.\")\n",
        "\n",
        "        for start_idx in range(0, video.shape[0], batch_size):\n",
        "            batch_frames = video[start_idx:start_idx + batch_size].float() / 255.0  # Scale frames\n",
        "\n",
        "            # Permute dimensions and apply normalization in batches\n",
        "            batch_frames = batch_frames.permute(0, 3, 1, 2)\n",
        "            transformed_batch = torch.stack([T.Normalize(mean=mean, std=std)(frame) for frame in batch_frames])\n",
        "\n",
        "            # Visualize transformed frames\n",
        "            num_frames = min(5, batch_frames.size(0))\n",
        "            fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "            if num_frames == 1:\n",
        "                axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
        "\n",
        "            for i in range(num_frames):\n",
        "                ax = axes[i]\n",
        "                frame = transformed_batch[i].permute(1, 2, 0).cpu().numpy()\n",
        "                frame = frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "                frame = np.clip(frame, 0, 1)\n",
        "                ax.imshow(frame)\n",
        "                ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            # Clear memory after processing each batch\n",
        "            del batch_frames, transformed_batch\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Load video using Decord\n",
        "def load_video_decord(filepath, max_frames=None):\n",
        "    video_reader = VideoReader(filepath, ctx=cpu(0))\n",
        "    frames = []\n",
        "\n",
        "    for i in range(len(video_reader)):\n",
        "        if max_frames and i >= max_frames:\n",
        "            break\n",
        "        frame = video_reader[i].asnumpy()\n",
        "        frames.append(frame)\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Process video with OpenCV\n",
        "    process_video_in_batches(video_path, batch_size=batch_size, max_frames=100)\n",
        "\n",
        "    # Process video with torchvision\n",
        "    load_and_transform_video_torchvision(video_path, batch_size=batch_size)\n",
        "\n",
        "    # Process video with Decord\n",
        "    decord_frames = load_video_decord(video_path, max_frames=100)\n",
        "\n",
        "    if decord_frames:\n",
        "        for start_idx in range(0, len(decord_frames), batch_size):\n",
        "            batch_frames = decord_frames[start_idx:start_idx + batch_size]\n",
        "            transformed_video = torch.stack([transform(frame) for frame in batch_frames])\n",
        "\n",
        "            # Visualize some frames\n",
        "            num_frames = min(5, len(batch_frames))\n",
        "            fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "            if num_frames == 1:\n",
        "                axes = [axes]  # Ensure axes is always a list for consistent indexing\n",
        "\n",
        "            for i in range(num_frames):\n",
        "                ax = axes[i]\n",
        "                transformed_frame = transformed_video[i].permute(1, 2, 0).numpy()  # Convert CHW to HWC\n",
        "                transformed_frame = transformed_frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "                transformed_frame = np.clip(transformed_frame, 0, 1)\n",
        "                ax.imshow(transformed_frame)\n",
        "                ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            # Clear memory after processing each batch\n",
        "            del batch_frames, transformed_video\n",
        "            torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTiOlH6NY1ss"
      },
      "outputs": [],
      "source": [
        "#runs out of RAM here\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "import cv2\n",
        "import av\n",
        "from video_dataset_custom import VideoDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "# Load video using OpenCV\n",
        "def load_video(video_path, max_frames=100):\n",
        "    if not os.path.exists(video_path):\n",
        "        raise RuntimeError(f\"File not found: {video_path}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or frame_count >= max_frames:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Define the transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Set the correct video path\n",
        "video_path = f'/content/{video_filename}'\n",
        "video_frames = load_video(video_path)\n",
        "print(f'Loaded {len(video_frames)} frames from the video.')  # Debug statement\n",
        "\n",
        "# Load and transform the video in smaller batches\n",
        "batch_size = 20\n",
        "video_frames = load_video(video_path, max_frames=batch_size)\n",
        "print(f'Loaded {len(video_frames)} frames from the video.')\n",
        "\n",
        "if video_frames:  # Ensure there are frames to process\n",
        "    for start_idx in range(0, len(video_frames), batch_size):\n",
        "        batch_frames = video_frames[start_idx:start_idx + batch_size]\n",
        "        transformed_video = torch.stack([transform(frame) for frame in batch_frames])\n",
        "\n",
        "        # Visualize some frames\n",
        "        fig, axes = plt.subplots(1, min(5, len(batch_frames)), figsize=(15, 5))\n",
        "        for i in range(min(5, len(batch_frames))):\n",
        "            ax = axes[i]\n",
        "            transformed_frame = transformed_video[i].permute(1, 2, 0).cpu().numpy()  # Convert CHW to HWC\n",
        "            transformed_frame = transformed_frame * std.numpy() + mean.numpy()  # De-normalize for visualization\n",
        "            transformed_frame = np.clip(transformed_frame, 0, 1)\n",
        "            ax.imshow(transformed_frame)\n",
        "            ax.set_title(f'Transformed Frame {i + start_idx}')\n",
        "            ax.axis('off')\n",
        "        plt.show()\n",
        "else:\n",
        "    print('No frames were loaded from the video.')\n",
        "\n",
        "# Load video using torchvision\n",
        "try:\n",
        "    video, audio, info = read_video(video_path, pts_unit='sec')\n",
        "    print(\"Video loaded using torchvision.\")\n",
        "except RuntimeError as e:\n",
        "    print(e)\n",
        "\n",
        "# Convert video frames to float and normalize if video is loaded\n",
        "if 'video' in locals():\n",
        "    video = video.permute(0, 3, 1, 2).float() / 255.0  # Convert from HWC to CHW and scale\n",
        "\n",
        "    # Apply transforms to each frame in batches to save memory\n",
        "    transformed_video = []\n",
        "    for start_idx in range(0, video.shape[0], batch_size):\n",
        "        batch_frames = video[start_idx:start_idx + batch_size]\n",
        "        transformed_batch = torch.stack([T.Normalize(mean=mean, std=std)(frame) for frame in batch_frames])\n",
        "        transformed_video.append(transformed_batch)\n",
        "\n",
        "    transformed_video = torch.cat(transformed_video)\n",
        "\n",
        "    # Display transformed video shape and a few frames\n",
        "    print(\"Transformed video shape:\", transformed_video.shape)\n",
        "    print(\"Transformed video:\", transformed_video[:3])\n",
        "\n",
        "    # Plot a few transformed frames\n",
        "    for i in range(3):\n",
        "        plt.figure()\n",
        "        frame = transformed_video[i].permute(1, 2, 0)  # Permute to HWC for plt.imshow\n",
        "        frame = frame.numpy()  # Convert to numpy array\n",
        "        frame = (frame - frame.min()) / (frame.max() - frame.min())  # Normalize for display\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print('No frames were loaded from the video.')\n",
        "\n",
        "# Load the video using torchvision\n",
        "video, audio, info = read_video(video_path)\n",
        "\n",
        "# Define transforms for torchvision\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Apply transforms to each frame\n",
        "transformed_video = torch.stack([transform(frame) for frame in video])\n",
        "\n",
        "# Display transformed video shape and a few frames\n",
        "print(\"Transformed video shape:\", transformed_video.shape)\n",
        "print(\"Transformed video:\", transformed_video[:3])\n",
        "\n",
        "# Plot a few transformed frames\n",
        "for i in range(3):\n",
        "    plt.figure()\n",
        "    frame = transformed_video[i].permute(1, 2, 0).numpy()  # Permute to HWC and convert to numpy\n",
        "    frame = (frame - frame.min()) / (frame.max() - frame.min())  # Normalize to [0, 1]\n",
        "    plt.imshow(frame)\n",
        "    plt.title(f'Transformed Frame {i}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Function to define transformations\n",
        "def get_video_transforms(crop_size=224, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "    return transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((crop_size, crop_size)),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "# Function to display video frames\n",
        "def display_video_frames(video_reader, num_frames=10):\n",
        "    \"\"\"Display video frames using Matplotlib and IPython.display\"\"\"\n",
        "    if not video_reader:\n",
        "        print(\"No video data to display\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "    for i in range(min(num_frames, len(video_reader))):\n",
        "        try:\n",
        "            frame = video_reader[i].asnumpy()\n",
        "            ax = axes[i] if num_frames > 1 else axes\n",
        "            ax.imshow(frame)\n",
        "            ax.set_title(f'Frame {i}')\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Clear the previous output before displaying the new frame\n",
        "            clear_output(wait=True)\n",
        "            display(plt.gcf())\n",
        "            plt.show()\n",
        "\n",
        "            # Pause to allow the frame to be displayed properly\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying frame {i}: {e}\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "# Load the video using decord\n",
        "def load_video_decord(filepath):\n",
        "    video_reader = VideoReader(filepath, ctx=cpu(0))\n",
        "    fps = video_reader.get_avg_fps()\n",
        "    return video_reader, fps\n",
        "\n",
        "# Main execution flow\n",
        "os.listdir()\n",
        "uploaded_file_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file path: {uploaded_file_path}\")\n",
        "\n",
        "# Construct the full file path\n",
        "fname = f'/content/{uploaded_file_path}'\n",
        "if not os.path.exists(fname):\n",
        "    warnings.warn(f'Video path not found: {fname}')\n",
        "else:\n",
        "    print(f\"File found at: {fname}\")\n",
        "\n",
        "    # Load the video\n",
        "    video_reader, fps = load_video_decord(fname)\n",
        "\n",
        "    # Display the first 10 frames\n",
        "    display_video_frames(video_reader, num_frames=1) #originally 10\n",
        "\n",
        "    # Apply transformations\n",
        "    transform = get_video_transforms()\n",
        "\n",
        "    # Step 6: Convert video frames to numpy arrays and apply transformations\n",
        "    transformed_frames = []\n",
        "    for frame in video_reader:\n",
        "        # Convert the frame to torch tensor and permute dimensions\n",
        "        frame_tensor = torch.tensor(frame.asnumpy()).permute(2, 0, 1).float() / 255.\n",
        "        # Apply the transformation\n",
        "        transformed_frame = transform(frame_tensor)\n",
        "        # Append to transformed_frames\n",
        "        transformed_frames.append(transformed_frame.numpy())  # No need for .numpy() here\n",
        "\n",
        "    # Step 7: Stack transformed frames into a single numpy array\n",
        "    transformed_video = np.stack(transformed_frames)\n",
        "\n",
        "    # Step 8: Print the transformed video shape\n",
        "    print(\"Transformed video shape:\", transformed_video.shape)\n",
        "\n",
        "    # Step 9: Print the transformed video\n",
        "    print(\"Transformed video:\")\n",
        "    print(transformed_video)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
        "    for i in range(10):\n",
        "        ax = axes[i]\n",
        "        transformed_frame = transformed_video[i].transpose(1, 2, 0)  # Convert CHW to HWC\n",
        "        transformed_frame = (transformed_frame * std + mean).clip(0, 1)  # De-normalize for visualization\n",
        "        ax.imshow(transformed_frame)\n",
        "        ax.set_title(f'Transformed Frame {i}')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Efficient Data Loading and Processing\n",
        "def make_videodataset(\n",
        "    data_paths,\n",
        "    batch_size,\n",
        "    frames_per_clip=8,\n",
        "    frame_step=4,\n",
        "    num_clips=1,\n",
        "    random_clip_sampling=True,\n",
        "    allow_clip_overlap=False,\n",
        "    filter_short_videos=False,\n",
        "    filter_long_videos=int(10**9),\n",
        "    transform=None,\n",
        "    shared_transform=None,\n",
        "    rank=0,\n",
        "    world_size=1,\n",
        "    datasets_weights=None,\n",
        "    collator=None,\n",
        "    drop_last=True,\n",
        "    num_workers=2,  # Set to 2 or 1 to reduce memory usage\n",
        "    pin_mem=True,\n",
        "    duration=None,\n",
        "    log_dir=None,\n",
        "):\n",
        "    dataset = VideoDataset(\n",
        "        data_paths=data_paths,\n",
        "        datasets_weights=datasets_weights,\n",
        "        frames_per_clip=frames_per_clip,\n",
        "        frame_step=frame_step,\n",
        "        num_clips=num_clips,\n",
        "        random_clip_sampling=random_clip_sampling,\n",
        "        allow_clip_overlap=allow_clip_overlap,\n",
        "        filter_short_videos=filter_short_videos,\n",
        "        filter_long_videos=filter_long_videos,\n",
        "        duration=duration,\n",
        "        shared_transform=shared_transform,\n",
        "        transform=transform)\n",
        "\n",
        "    print('VideoDataset dataset created')\n",
        "    if datasets_weights is not None:\n",
        "        dist_sampler = DistributedWeightedSampler(\n",
        "            dataset.sample_weights,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "    else:\n",
        "        dist_sampler = DistributedSampler(\n",
        "            dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        collate_fn=collator,\n",
        "        sampler=dist_sampler,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=drop_last,\n",
        "        pin_memory=pin_mem,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=num_workers > 0)\n",
        "    print('VideoDataset unsupervised data loader created')\n",
        "\n",
        "    return dataset, data_loader, dist_sampler\n",
        "\n",
        "# Use the correct video path\n",
        "dataset, data_loader, dist_sampler = make_videodataset(\n",
        "    data_paths=[fname],\n",
        "    batch_size=4,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Efficiently Process and Transform Video Frames\n",
        "for i, batch in enumerate(data_loader):\n",
        "    print(f\"Processing batch {i}\")\n",
        "    transformed_frames = batch.permute(0, 2, 3, 1).numpy()  # Convert from tensor to numpy for display\n",
        "    for frame in transformed_frames:\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    if i >= 2:  # Limit to processing first 2 batches for demonstration\n",
        "        break\n",
        "\n",
        "\n",
        "# Path to the video file\n",
        "video_path = 'path/to/video.mp4'\n",
        "\n",
        "# Load the video\n",
        "video, audio, info = read_video(video_path)\n",
        "\n",
        "# Display video information\n",
        "print(\"Video shape:\", video.shape)\n",
        "print(\"Audio shape:\", audio.shape)\n",
        "print(\"Video info:\", info)\n",
        "\n",
        "# Transforms\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Apply transforms to each frame\n",
        "transformed_video = torch.stack([transform(frame) for frame in video])\n",
        "\n",
        "# Display transformed video shape and a few frames\n",
        "print(\"Transformed video shape:\", transformed_video.shape)\n",
        "print(\"Transformed video:\", transformed_video[:3])\n",
        "\n",
        "# Plot a few transformed frames\n",
        "for i in range(3):\n",
        "    plt.figure()\n",
        "    frame = transformed_video[i].permute(1, 2, 0)  # Permute to HWC for plt.imshow\n",
        "    frame = frame.numpy()  # Convert to numpy array\n",
        "    frame = (frame - frame.min()) / (frame.max() - frame.min())  # Normalize to [0, 1]\n",
        "    plt.imshow(frame)\n",
        "    plt.title(f'Transformed Frame {i}')\n",
        "    plt.show()  # Ensure the plot is displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inNWQeUsx5Fq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision import transforms\n",
        "from video_dataset_custom import VideoDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Function to define transformations\n",
        "def get_video_transforms(crop_size=224, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "    return transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((crop_size, crop_size)),\n",
        "        transforms.CenterCrop(crop_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "def load_video_decord(sample, filter_long_videos=int(10**9)):\n",
        "    \"\"\"Load video content using Decord\"\"\"\n",
        "    fname = sample\n",
        "    if not os.path.exists(fname):\n",
        "        warnings.warn(f'video path not found {fname=}')\n",
        "        return [], None\n",
        "\n",
        "    _fsize = os.path.getsize(fname)\n",
        "    if _fsize < 1 * 1024:  # avoid hanging issue\n",
        "        warnings.warn(f'video too short {fname=}')\n",
        "        return [], None\n",
        "    if _fsize > filter_long_videos:\n",
        "        warnings.warn(f'skipping long video of size {_fsize=} (bytes)')\n",
        "        return [], None\n",
        "\n",
        "    try:\n",
        "        vr = VideoReader(fname, num_threads=-1, ctx=cpu(0))\n",
        "        print(f\"Video loaded successfully: {fname}\")\n",
        "        return vr, vr.get_avg_fps()\n",
        "    except Exception as e:\n",
        "        warnings.warn(f'error loading video {fname=}: {e}')\n",
        "        return [], None\n",
        "\n",
        "def display_evenly_spaced_frames(video_reader, num_frames=8):\n",
        "    \"\"\"Display N evenly spaced-out frames using Matplotlib\"\"\"\n",
        "    if not video_reader:\n",
        "        print(\"No video data to display\")\n",
        "        return\n",
        "\n",
        "    total_frames = len(video_reader)\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    print(f\"Total frames in video: {total_frames}\")\n",
        "    print(f\"Selected frame indices: {indices}\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        try:\n",
        "            frame = video_reader[idx].asnumpy()\n",
        "            ax = axes[i] if num_frames > 1 else axes\n",
        "            ax.imshow(frame)\n",
        "            ax.set_title(f'Frame {idx}')\n",
        "            ax.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying frame {idx}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def display_transformed_frames(transformed_frames, mean, std, num_frames=8):\n",
        "    \"\"\"Display N transformed frames using Matplotlib\"\"\"\n",
        "    total_frames = len(transformed_frames)\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        try:\n",
        "            transformed_frame = transformed_frames[idx].transpose(1, 2, 0)  # Convert CHW to HWC\n",
        "            transformed_frame = (transformed_frame * std + mean).clip(0, 1)  # De-normalize for visualization\n",
        "            ax = axes[i] if num_frames > 1 else axes\n",
        "            ax.imshow(transformed_frame)\n",
        "            ax.set_title(f'Transformed Frame {idx}')\n",
        "            ax.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying transformed frame {idx}: {e}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'uploaded' is a dictionary of uploaded files\n",
        "file_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file path: {file_path}\")\n",
        "\n",
        "# Construct the file path\n",
        "fname = f'/content/data/{file_path}'\n",
        "if not os.path.exists(fname):\n",
        "    warnings.warn(f'video path not found {fname=}')\n",
        "else:\n",
        "    print(f\"File found at: {fname}\")\n",
        "    # Proceed with your video processing code\n",
        "\n",
        "# Load the video\n",
        "video_reader, fps = load_video_decord(fname)\n",
        "\n",
        "# Display the first 8 frames\n",
        "display_evenly_spaced_frames(video_reader, num_frames=8)\n",
        "\n",
        "# Apply transformations\n",
        "transform = get_video_transforms()\n",
        "\n",
        "# Convert video frames to numpy arrays and apply transformations\n",
        "transformed_frames = []\n",
        "for frame in video_reader:\n",
        "    frame_tensor = torch.tensor(frame.asnumpy()).permute(2, 0, 1).float() / 255.\n",
        "    transformed_frame = transform(frame_tensor)\n",
        "    transformed_frames.append(transformed_frame.numpy())\n",
        "\n",
        "# Display the transformed frames\n",
        "display_transformed_frames(transformed_frames, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), num_frames=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLC7P_nceXTc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from decord import VideoReader, cpu\n",
        "from video_dataset_custom import VideoDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import video_transforms_custom as video_transforms\n",
        "import volume_transforms_custom as volume_transforms\n",
        "\n",
        "# Step 1: Get the file name\n",
        "video_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Print the uploaded file name\n",
        "print(f'Uploaded file: {video_filename}')\n",
        "\n",
        "# Step 2: Define the video path in Colab environment\n",
        "video_path = f'/content/{video_filename}'\n",
        "\n",
        "# Step 3: Check if the file exists\n",
        "if os.path.exists(video_path):\n",
        "    print(f\"{video_path} exists!\")\n",
        "else:\n",
        "    print(f\"{video_path} does not exist.\")\n",
        "\n",
        "# Step 4: Manually check the content directory\n",
        "print(os.listdir('/content'))\n",
        "\n",
        "# Define make_transforms function and VideoTransform class\n",
        "def make_transforms(\n",
        "    training=False,\n",
        "    random_horizontal_flip=False,\n",
        "    random_resize_aspect_ratio=(0.99999, 1.00001),\n",
        "    random_resize_scale=(0.3, 1.0), #find out what 0.3 does\n",
        "    auto_augment=False,\n",
        "    motion_shift=False,\n",
        "    crop_size=224,\n",
        "    normalize=((0.485, 0.456, 0.406),\n",
        "               (0.229, 0.224, 0.225))\n",
        "):\n",
        "\n",
        "    if not training:\n",
        "        print('Making EvalVideoTransform, multi-view')\n",
        "        _frames_augmentation = EvalVideoTransform(\n",
        "            num_views_per_clip=1,\n",
        "            short_side_size=crop_size,\n",
        "            normalize=normalize,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        _frames_augmentation = VideoTransform(\n",
        "            training=training,\n",
        "            random_horizontal_flip=random_horizontal_flip,\n",
        "            random_resize_aspect_ratio=random_resize_aspect_ratio,\n",
        "            random_resize_scale=random_resize_scale,\n",
        "            auto_augment=auto_augment,\n",
        "            motion_shift=motion_shift,\n",
        "            crop_size=crop_size,\n",
        "            normalize=normalize,\n",
        "        )\n",
        "    return _frames_augmentation\n",
        "\n",
        "\n",
        "class VideoTransform(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        training=False,\n",
        "        random_horizontal_flip=False,\n",
        "        random_resize_aspect_ratio=(0.99999, 1.00001),\n",
        "        random_resize_scale=(0.3, 1.0), #find out what 0.3 does\n",
        "        auto_augment=False,\n",
        "        motion_shift=False,\n",
        "        crop_size=224,\n",
        "        normalize=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ):\n",
        "        self.training = training\n",
        "        self.random_horizontal_flip = random_horizontal_flip\n",
        "        self.random_resize_aspect_ratio = random_resize_aspect_ratio\n",
        "        self.random_resize_scale = random_resize_scale\n",
        "        self.auto_augment = auto_augment\n",
        "        self.motion_shift = motion_shift\n",
        "        self.crop_size = crop_size\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Define your transformation pipeline using video_transforms and volume_transforms\n",
        "\n",
        "    def eval_transform(self, frame_tensor):\n",
        "        # Apply your transformations here\n",
        "        transformed_frame = frame_tensor.clone().cpu().numpy()  # Convert tensor to numpy array\n",
        "        # Apply transformations using video_transforms or other libraries\n",
        "        # transformed_frame = video_transforms.Compose([\n",
        "        #     video_transforms.Resize((self.crop_size, self.crop_size)),\n",
        "        #     video_transforms.CenterCrop(self.crop_size),\n",
        "        #     # Add more transformations as needed\n",
        "        # ])(Image.fromarray(transformed_frame.transpose(1, 2, 0)))  # Convert back to PIL Image\n",
        "\n",
        "        # Ensure transformed_frame is a numpy array\n",
        "        transformed_frame = np.array(transformed_frame)\n",
        "\n",
        "        return transformed_frame\n",
        "\n",
        "# Create the transform object\n",
        "transform = make_transforms(crop_size=224)\n",
        "\n",
        "# Define make_videodataset function\n",
        "def make_videodataset(\n",
        "    data_paths,\n",
        "    batch_size,\n",
        "    frames_per_clip=8,\n",
        "    frame_step=4,\n",
        "    num_clips=1,\n",
        "    random_clip_sampling=True,\n",
        "    allow_clip_overlap=False,\n",
        "    filter_short_videos=False,\n",
        "    filter_long_videos=int(10**9),\n",
        "    transform=None,\n",
        "    shared_transform=None,\n",
        "    rank=0,\n",
        "    world_size=1,\n",
        "    datasets_weights=None,\n",
        "    collator=None,\n",
        "    drop_last=True,\n",
        "    num_workers=2, #originally 10\n",
        "    pin_mem=True,\n",
        "    duration=None,\n",
        "    log_dir=None,\n",
        "):\n",
        "    dataset = VideoDataset(\n",
        "        data_paths=data_paths,\n",
        "        datasets_weights=datasets_weights,\n",
        "        frames_per_clip=frames_per_clip,\n",
        "        frame_step=frame_step,\n",
        "        num_clips=num_clips,\n",
        "        random_clip_sampling=random_clip_sampling,\n",
        "        allow_clip_overlap=allow_clip_overlap,\n",
        "        filter_short_videos=filter_short_videos,\n",
        "        filter_long_videos=filter_long_videos,\n",
        "        duration=duration,\n",
        "        shared_transform=shared_transform,\n",
        "        transform=transform)\n",
        "\n",
        "    print('VideoDataset dataset created')\n",
        "    if datasets_weights is not None:\n",
        "        dist_sampler = DistributedWeightedSampler(\n",
        "            dataset.sample_weights,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "    else:\n",
        "        dist_sampler = DistributedSampler(\n",
        "            dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        collate_fn=collator,\n",
        "        sampler=dist_sampler,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=drop_last,\n",
        "        pin_memory=pin_mem,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=num_workers > 0)\n",
        "    print('VideoDataset unsupervised data loader created')\n",
        "\n",
        "    return dataset, data_loader, dist_sampler\n",
        "\n",
        "\n",
        "# Use the correct video path\n",
        "dataset, data_loader, dist_sampler = make_videodataset(\n",
        "    data_paths=[video_path],\n",
        "    batch_size=4,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Step 5: Load the video using decord\n",
        "vr = VideoReader(video_path, ctx=cpu(0))\n",
        "\n",
        "# Step 6: Convert video frames to numpy arrays\n",
        "video_frames = [frame.asnumpy() for frame in vr]\n",
        "\n",
        "# Step 7: Stack frames into a single numpy array\n",
        "video = np.stack(video_frames)\n",
        "\n",
        "# Step 8: Apply the transformation to each frame of the video\n",
        "transformed_frames = []\n",
        "for frame in video:\n",
        "    # Convert the frame to torch tensor and permute dimensions\n",
        "    frame_tensor = torch.tensor(frame).permute(2, 0, 1).float() / 255.\n",
        "    # Apply the transformation\n",
        "    transformed_frame = transform.eval_transform(frame_tensor)\n",
        "    # Append to transformed_frames\n",
        "    transformed_frames.append(transformed_frame)  # No need for .numpy() here\n",
        "\n",
        "# Step 9: Stack transformed frames into a single numpy array\n",
        "transformed_video = np.stack(transformed_frames)\n",
        "\n",
        "# Step 10: Print the transformed video shape\n",
        "print(\"Transformed video shape:\", transformed_video.shape)\n",
        "\n",
        "# Step 11: Print the transformed video\n",
        "print(\"Transformed video:\")\n",
        "print(transformed_video)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX54YjX3akpp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import video_transforms_custom as video_transforms\n",
        "import volume_transforms_custom as volume_transforms\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "from video_dataset_custom import VideoDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "# Step 2: Get the file name\n",
        "video_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Print the uploaded file name\n",
        "print(f'Uploaded file: {video_filename}')\n",
        "\n",
        "# Step 3: Define the video path in Colab environment\n",
        "video_path = f'/content/{video_filename}'\n",
        "\n",
        "# Step 4: Check if the file exists\n",
        "if os.path.exists(video_path):\n",
        "    print(f\"{video_path} exists!\")\n",
        "else:\n",
        "    print(f\"{video_path} does not exist.\")\n",
        "\n",
        "# Step 5: Manually check the content directory\n",
        "print(os.listdir('/content'))\n",
        "\n",
        "\n",
        "# Define make_transforms function and VideoTransform class\n",
        "def make_transforms(\n",
        "    training=True,\n",
        "    random_horizontal_flip=True,\n",
        "    random_resize_aspect_ratio=(3/4, 4/3),\n",
        "    random_resize_scale=(0.3, 1.0),\n",
        "    auto_augment=False,\n",
        "    motion_shift=False,\n",
        "    crop_size=224,\n",
        "    normalize=((0.485, 0.456, 0.406),\n",
        "               (0.229, 0.224, 0.225))\n",
        "):\n",
        "\n",
        "    if not training:\n",
        "        print('Making EvalVideoTransform, multi-view')\n",
        "        _frames_augmentation = EvalVideoTransform(\n",
        "            num_views_per_clip=1,\n",
        "            short_side_size=crop_size,\n",
        "            normalize=normalize,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        _frames_augmentation = VideoTransform(\n",
        "            training=training,\n",
        "            random_horizontal_flip=random_horizontal_flip,\n",
        "            random_resize_aspect_ratio=random_resize_aspect_ratio,\n",
        "            random_resize_scale=random_resize_scale,\n",
        "            auto_augment=auto_augment,\n",
        "            motion_shift=motion_shift,\n",
        "            crop_size=crop_size,\n",
        "            normalize=normalize,\n",
        "        )\n",
        "    return _frames_augmentation\n",
        "\n",
        "\n",
        "class VideoTransform(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        training=True, #change to false\n",
        "        random_horizontal_flip=True, #change to false\n",
        "        random_resize_aspect_ratio=(0.99999, 1.00001),\n",
        "        random_resize_scale=(0.3, 1.0), #find out what 0.3 does\n",
        "        auto_augment=False,\n",
        "        motion_shift=False,\n",
        "        crop_size=224,\n",
        "        normalize=((0.485, 0.456, 0.406),\n",
        "                   (0.229, 0.224, 0.225))\n",
        "    ):\n",
        "\n",
        "        self.training = training\n",
        "\n",
        "        short_side_size = int(crop_size * 256 / 224)\n",
        "        self.eval_transform = video_transforms.Compose([\n",
        "            video_transforms.Resize(short_side_size, interpolation='bilinear'),\n",
        "            video_transforms.CenterCrop(size=(crop_size, crop_size)),\n",
        "            volume_transforms.ClipToTensor(),\n",
        "            video_transforms.Normalize(mean=normalize[0], std=normalize[1])\n",
        "        ])\n",
        "\n",
        "        self.random_horizontal_flip = random_horizontal_flip\n",
        "        self.random_resize_aspect_ratio = random_resize_aspect_ratio\n",
        "        self.random_resize_scale = random_resize_scale\n",
        "        self.auto_augment = auto_augment\n",
        "        self.motion_shift = motion_shift\n",
        "        self.crop_size = crop_size\n",
        "        self.normalize = torch.tensor(normalize)\n",
        "\n",
        "        self.autoaug_transform = video_transforms.create_random_augment(\n",
        "            input_size=(crop_size, crop_size),\n",
        "            auto_augment='rand-m7-n4-mstd0.5-inc1',\n",
        "            interpolation='bicubic',\n",
        "        )\n",
        "\n",
        "        self.spatial_transform = video_transforms.random_resized_crop_with_shift \\\n",
        "            if motion_shift else video_transforms.random_resized_crop\n",
        "\n",
        "# Create the transform object\n",
        "transform = make_transforms(crop_size=224)\n",
        "\n",
        "# Define make_videodataset function\n",
        "def make_videodataset(\n",
        "    data_paths,\n",
        "    batch_size,\n",
        "    frames_per_clip=8,\n",
        "    frame_step=4,\n",
        "    num_clips=1,\n",
        "    random_clip_sampling=True,\n",
        "    allow_clip_overlap=False,\n",
        "    filter_short_videos=False,\n",
        "    filter_long_videos=int(10**9),\n",
        "    transform=None,\n",
        "    shared_transform=None,\n",
        "    rank=0,\n",
        "    world_size=1,\n",
        "    datasets_weights=None,\n",
        "    collator=None,\n",
        "    drop_last=True,\n",
        "    num_workers=2, #originally 10\n",
        "    pin_mem=True,\n",
        "    duration=None,\n",
        "    log_dir=None,\n",
        "):\n",
        "    dataset = VideoDataset(\n",
        "        data_paths=data_paths,\n",
        "        datasets_weights=datasets_weights,\n",
        "        frames_per_clip=frames_per_clip,\n",
        "        frame_step=frame_step,\n",
        "        num_clips=num_clips,\n",
        "        random_clip_sampling=random_clip_sampling,\n",
        "        allow_clip_overlap=allow_clip_overlap,\n",
        "        filter_short_videos=filter_short_videos,\n",
        "        filter_long_videos=filter_long_videos,\n",
        "        duration=duration,\n",
        "        shared_transform=shared_transform,\n",
        "        transform=transform)\n",
        "\n",
        "    print('VideoDataset dataset created')\n",
        "    if datasets_weights is not None:\n",
        "        dist_sampler = DistributedWeightedSampler(\n",
        "            dataset.sample_weights,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "    else:\n",
        "        dist_sampler = DistributedSampler(\n",
        "            dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank,\n",
        "            shuffle=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        collate_fn=collator,\n",
        "        sampler=dist_sampler,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=drop_last,\n",
        "        pin_memory=pin_mem,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=num_workers > 0)\n",
        "    print('VideoDataset unsupervised data loader created')\n",
        "\n",
        "    return dataset, data_loader, dist_sampler\n",
        "\n",
        "# Use the correct video path\n",
        "dataset, data_loader, dist_sampler = make_videodataset(\n",
        "    data_paths=[video_path],\n",
        "    batch_size=4,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Load the video using decord\n",
        "vr = VideoReader(video_path, ctx=cpu(0))\n",
        "\n",
        "# Step 3: Convert video frames to numpy arrays\n",
        "video_frames = [frame.asnumpy() for frame in vr]\n",
        "\n",
        "# Step 4: Stack frames into a single numpy array\n",
        "video = np.stack(video_frames)\n",
        "\n",
        "\n",
        "transform = make_transforms(crop_size=224)\n",
        "\n",
        "# Step 7: Apply the transformation to each frame of the video\n",
        "transformed_frames = []\n",
        "for frame in video:\n",
        "    # Convert the frame to torch tensor and permute dimensions\n",
        "    frame_tensor = torch.tensor(frame).permute(2, 0, 1).float() / 255.\n",
        "    # Apply the transformation\n",
        "    transformed_frame = transform.eval_transform(frame_tensor)\n",
        "    # Convert back to numpy array and append to transformed_frames\n",
        "    transformed_frames.append(transformed_frame.numpy())\n",
        "\n",
        "# Step 8: Stack transformed frames into a single numpy array\n",
        "transformed_video = np.stack(transformed_frames)\n",
        "\n",
        "# Step 9: Print the transformed video shape\n",
        "print(\"Transformed video shape:\", transformed_video.shape)\n",
        "\n",
        "# Step 10: Print the transformed video\n",
        "print(\"Transformed video:\")\n",
        "print(transformed_video)\n",
        "\n",
        "\n",
        "\n",
        "# # Check if the dataset and data loader are created successfully\n",
        "# print(\"Dataset and data loader created successfully.\")\n",
        "\n",
        "# # Load the video using decord\n",
        "# vr = VideoReader(video_path, ctx=cpu(0))\n",
        "\n",
        "# # Convert video to tensor\n",
        "# video_frames = [torch.tensor(frame.asnumpy()) for frame in vr]\n",
        "\n",
        "# # Stack frames into a single tensor\n",
        "# video = torch.stack(video_frames)\n",
        "\n",
        "# # Transform the video\n",
        "# transformed_video = transform(video)\n",
        "\n",
        "# # Print the transformed video tensor\n",
        "# print(\"Transformed video tensor:\")\n",
        "# print(transformed_video)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXlvw2vtjC7YpfSdvO9I4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}